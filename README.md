# LLM Evaluation Project

This project was built to evaluate the performance of LLMs, whereby testing different system prompts on these models helps us to figure out which LLM is the best fit for a startup company.

## Requirements
- Create a full stack web app with an interface for inputting prompts and viewing responses from multiple LLMs side-by-side

- Integrate metrics such as accuracy, relevancy, and response time for each LLM

- Store user prompts and experiment results in a database

- Implement an analytics dashboard for visualizing performance metrics for different prompts and LLMs

